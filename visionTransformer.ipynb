{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrivi34/VisionTransformer_ViT/blob/main/visionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEkfJNn3nlxk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEK5MAGEfL0Z",
        "outputId": "9f7b6c32-f9eb-4b5f-8ed2-d82383652079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFj2V_TMaXc2",
        "outputId": "0ad0b02a-8954-4205-ff65-1570b45dff4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ8dUBj6aWdh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47l8XuKwfLGm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_BfU9gb7ajIl",
        "outputId": "ad26a649-9d75-48c0-f161-9f4a9c5f795e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6ec807ab2350>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/universe.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
          ]
        }
      ],
      "source": [
        "image_path = '/content/drive/My Drive/universe.jpg'\n",
        "img = Image.open(image_path)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge Detection Using Image Derivatives\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_image(image_path, resize_size=(224, 224)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(resize_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def sobel_filter(image_tensor):\n",
        "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
        "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
        "    filters = torch.stack([sobel_x, sobel_y]).unsqueeze(1)  # Shape: [2, 1, 3, 3]\n",
        "\n",
        "    gray_scaled = image_tensor.mean(dim=1, keepdim=True)  # Convert to grayscale\n",
        "    edges = F.conv2d(gray_scaled, filters, padding=1)\n",
        "    return edges.abs().sum(dim=1).unsqueeze(1)  # Sum along x and y axis\n",
        "\n",
        "# Example usage\n",
        "image_tensor = load_image(image_path)\n",
        "edges = sobel_filter(image_tensor)\n"
      ],
      "metadata": {
        "id": "3xLs29RwgaRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weighted_patches(image_tensor, edges, patch_size=16):\n",
        "    b, c, h, w = image_tensor.shape\n",
        "    image_patches = image_tensor.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    image_patches = image_patches.contiguous().view(b, c, -1, patch_size, patch_size)  # b, c, num_patches, patch_size, patch_size\n",
        "\n",
        "    edge_patches = edges.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    edge_patches = edge_patches.contiguous().view(b, 1, -1, patch_size, patch_size)  # b, 1, num_patches, patch_size, patch_size\n",
        "\n",
        "    # Compute weights for each patch based on edge intensity\n",
        "    patch_weights = edge_patches.mean(dim=[3, 4])  # Average intensity of edges in each patch\n",
        "\n",
        "    return image_patches, patch_weights\n",
        "\n",
        "image_patches, patch_weights = create_weighted_patches(image_tensor, edges)\n"
      ],
      "metadata": {
        "id": "X6NN7DfHgexs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dL3ztQXap_0",
        "outputId": "1beb5f34-b63d-423f-9e2b-fe9f7948b4db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# resize to imagenet size\n",
        "transform = Compose([Resize((224, 224)), ToTensor()])\n",
        "x = transform(img)\n",
        "x = x.unsqueeze(0) # add batch dim\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNhZJEJDa_sG"
      },
      "outputs": [],
      "source": [
        "patch_size = 16 # 16 pixels\n",
        "pathes = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z15s6tebJKm",
        "outputId": "8f363570-3670-4583-a7aa-d21e3e4826a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "PatchEmbedding()(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63QVe4vobLlh",
        "outputId": "e05b42d4-7889-4626-ae02-31bcaab2050f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # break-down the image in s1 x s2 patches and flat them\n",
        "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "PatchEmbedding()(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhxw1FacbqoP",
        "outputId": "76e26ef2-0c87-4dae-f0e4-d35b1b024539"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 196, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "PatchEmbedding()(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1-hOdtMb2xW"
      },
      "source": [
        "CLS Token\n",
        "\n",
        "Next step is to add the cls token and the position embedding. The cls token is just a number placed in from of each sequence (of projected patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mi8a16Db19u"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ZgBQz2bx77",
        "outputId": "09301442-2431-44d3-96d3-9d9ac49088e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.proj(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        return x\n",
        "\n",
        "PatchEmbedding()(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#modified patch embedding for new method\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
        "\n",
        "    def forward(self, x: Tensor, patch_weights: Tensor = None) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply patch weights if provided\n",
        "        if patch_weights is not None:\n",
        "            # Ensure patch weights are properly shaped and broadcastable\n",
        "            patch_weights = patch_weights.view(b, -1, 1)\n",
        "            x *= patch_weights  # Element-wise multiplication of weights\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.positions\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hNujh7SehVPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_patch_weights(x, patch_size=16):\n",
        "    # Implement edge detection and calculate weights for each patch\n",
        "    # For simplicity, returning a tensor of ones (dummy weights)\n",
        "    num_patches = (224 // patch_size) ** 2\n",
        "    return torch.ones((x.size(0), num_patches), device=x.device)\n"
      ],
      "metadata": {
        "id": "rBlsnhZjiTZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kcs5cmjoillw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llm8VLlwcAdc",
        "outputId": "17c98a8f-b122-4aeb-f3cd-5b1c7a5aa78f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Transformer Ateention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "patches_embedded = PatchEmbedding()(x)\n",
        "MultiHeadAttention()(patches_embedded).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbHio1ltcPHt"
      },
      "outputs": [],
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1KR1uEO4skZ"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjNDdwcd4u7G"
      },
      "outputs": [],
      "source": [
        "#Transformer Encoder\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 768,\n",
        "                 drop_p: float = 0.,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in93JHzg49Tq",
        "outputId": "64b5c554-ed30-4e4c-a7d9-7593572803e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "patches_embedded = PatchEmbedding()(x)\n",
        "TransformerEncoderBlock()(patches_embedded).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5HXpNn65A3G"
      },
      "outputs": [],
      "source": [
        "#Transformer\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB5sspxM5HFi"
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57D2adsl5Sj6"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                in_channels: int = 3,\n",
        "                patch_size: int = 16,\n",
        "                emb_size: int = 768,\n",
        "                img_size: int = 224,\n",
        "                depth: int = 12,\n",
        "                n_classes: int = 1000,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr-ZxMMC5UpS",
        "outputId": "2231fd27-90f9-4bd3-f309-a04f3debe1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
            "         Rearrange-2             [-1, 196, 768]               0\n",
            "    PatchEmbedding-3             [-1, 197, 768]               0\n",
            "         LayerNorm-4             [-1, 197, 768]           1,536\n",
            "            Linear-5            [-1, 197, 2304]       1,771,776\n",
            "           Dropout-6          [-1, 8, 197, 197]               0\n",
            "            Linear-7             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-8             [-1, 197, 768]               0\n",
            "           Dropout-9             [-1, 197, 768]               0\n",
            "      ResidualAdd-10             [-1, 197, 768]               0\n",
            "        LayerNorm-11             [-1, 197, 768]           1,536\n",
            "           Linear-12            [-1, 197, 3072]       2,362,368\n",
            "             GELU-13            [-1, 197, 3072]               0\n",
            "          Dropout-14            [-1, 197, 3072]               0\n",
            "           Linear-15             [-1, 197, 768]       2,360,064\n",
            "          Dropout-16             [-1, 197, 768]               0\n",
            "      ResidualAdd-17             [-1, 197, 768]               0\n",
            "        LayerNorm-18             [-1, 197, 768]           1,536\n",
            "           Linear-19            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-20          [-1, 8, 197, 197]               0\n",
            "           Linear-21             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-22             [-1, 197, 768]               0\n",
            "          Dropout-23             [-1, 197, 768]               0\n",
            "      ResidualAdd-24             [-1, 197, 768]               0\n",
            "        LayerNorm-25             [-1, 197, 768]           1,536\n",
            "           Linear-26            [-1, 197, 3072]       2,362,368\n",
            "             GELU-27            [-1, 197, 3072]               0\n",
            "          Dropout-28            [-1, 197, 3072]               0\n",
            "           Linear-29             [-1, 197, 768]       2,360,064\n",
            "          Dropout-30             [-1, 197, 768]               0\n",
            "      ResidualAdd-31             [-1, 197, 768]               0\n",
            "        LayerNorm-32             [-1, 197, 768]           1,536\n",
            "           Linear-33            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-34          [-1, 8, 197, 197]               0\n",
            "           Linear-35             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-36             [-1, 197, 768]               0\n",
            "          Dropout-37             [-1, 197, 768]               0\n",
            "      ResidualAdd-38             [-1, 197, 768]               0\n",
            "        LayerNorm-39             [-1, 197, 768]           1,536\n",
            "           Linear-40            [-1, 197, 3072]       2,362,368\n",
            "             GELU-41            [-1, 197, 3072]               0\n",
            "          Dropout-42            [-1, 197, 3072]               0\n",
            "           Linear-43             [-1, 197, 768]       2,360,064\n",
            "          Dropout-44             [-1, 197, 768]               0\n",
            "      ResidualAdd-45             [-1, 197, 768]               0\n",
            "        LayerNorm-46             [-1, 197, 768]           1,536\n",
            "           Linear-47            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-48          [-1, 8, 197, 197]               0\n",
            "           Linear-49             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-50             [-1, 197, 768]               0\n",
            "          Dropout-51             [-1, 197, 768]               0\n",
            "      ResidualAdd-52             [-1, 197, 768]               0\n",
            "        LayerNorm-53             [-1, 197, 768]           1,536\n",
            "           Linear-54            [-1, 197, 3072]       2,362,368\n",
            "             GELU-55            [-1, 197, 3072]               0\n",
            "          Dropout-56            [-1, 197, 3072]               0\n",
            "           Linear-57             [-1, 197, 768]       2,360,064\n",
            "          Dropout-58             [-1, 197, 768]               0\n",
            "      ResidualAdd-59             [-1, 197, 768]               0\n",
            "        LayerNorm-60             [-1, 197, 768]           1,536\n",
            "           Linear-61            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-62          [-1, 8, 197, 197]               0\n",
            "           Linear-63             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-64             [-1, 197, 768]               0\n",
            "          Dropout-65             [-1, 197, 768]               0\n",
            "      ResidualAdd-66             [-1, 197, 768]               0\n",
            "        LayerNorm-67             [-1, 197, 768]           1,536\n",
            "           Linear-68            [-1, 197, 3072]       2,362,368\n",
            "             GELU-69            [-1, 197, 3072]               0\n",
            "          Dropout-70            [-1, 197, 3072]               0\n",
            "           Linear-71             [-1, 197, 768]       2,360,064\n",
            "          Dropout-72             [-1, 197, 768]               0\n",
            "      ResidualAdd-73             [-1, 197, 768]               0\n",
            "        LayerNorm-74             [-1, 197, 768]           1,536\n",
            "           Linear-75            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-76          [-1, 8, 197, 197]               0\n",
            "           Linear-77             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-78             [-1, 197, 768]               0\n",
            "          Dropout-79             [-1, 197, 768]               0\n",
            "      ResidualAdd-80             [-1, 197, 768]               0\n",
            "        LayerNorm-81             [-1, 197, 768]           1,536\n",
            "           Linear-82            [-1, 197, 3072]       2,362,368\n",
            "             GELU-83            [-1, 197, 3072]               0\n",
            "          Dropout-84            [-1, 197, 3072]               0\n",
            "           Linear-85             [-1, 197, 768]       2,360,064\n",
            "          Dropout-86             [-1, 197, 768]               0\n",
            "      ResidualAdd-87             [-1, 197, 768]               0\n",
            "        LayerNorm-88             [-1, 197, 768]           1,536\n",
            "           Linear-89            [-1, 197, 2304]       1,771,776\n",
            "          Dropout-90          [-1, 8, 197, 197]               0\n",
            "           Linear-91             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-92             [-1, 197, 768]               0\n",
            "          Dropout-93             [-1, 197, 768]               0\n",
            "      ResidualAdd-94             [-1, 197, 768]               0\n",
            "        LayerNorm-95             [-1, 197, 768]           1,536\n",
            "           Linear-96            [-1, 197, 3072]       2,362,368\n",
            "             GELU-97            [-1, 197, 3072]               0\n",
            "          Dropout-98            [-1, 197, 3072]               0\n",
            "           Linear-99             [-1, 197, 768]       2,360,064\n",
            "         Dropout-100             [-1, 197, 768]               0\n",
            "     ResidualAdd-101             [-1, 197, 768]               0\n",
            "       LayerNorm-102             [-1, 197, 768]           1,536\n",
            "          Linear-103            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-104          [-1, 8, 197, 197]               0\n",
            "          Linear-105             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-106             [-1, 197, 768]               0\n",
            "         Dropout-107             [-1, 197, 768]               0\n",
            "     ResidualAdd-108             [-1, 197, 768]               0\n",
            "       LayerNorm-109             [-1, 197, 768]           1,536\n",
            "          Linear-110            [-1, 197, 3072]       2,362,368\n",
            "            GELU-111            [-1, 197, 3072]               0\n",
            "         Dropout-112            [-1, 197, 3072]               0\n",
            "          Linear-113             [-1, 197, 768]       2,360,064\n",
            "         Dropout-114             [-1, 197, 768]               0\n",
            "     ResidualAdd-115             [-1, 197, 768]               0\n",
            "       LayerNorm-116             [-1, 197, 768]           1,536\n",
            "          Linear-117            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-118          [-1, 8, 197, 197]               0\n",
            "          Linear-119             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-120             [-1, 197, 768]               0\n",
            "         Dropout-121             [-1, 197, 768]               0\n",
            "     ResidualAdd-122             [-1, 197, 768]               0\n",
            "       LayerNorm-123             [-1, 197, 768]           1,536\n",
            "          Linear-124            [-1, 197, 3072]       2,362,368\n",
            "            GELU-125            [-1, 197, 3072]               0\n",
            "         Dropout-126            [-1, 197, 3072]               0\n",
            "          Linear-127             [-1, 197, 768]       2,360,064\n",
            "         Dropout-128             [-1, 197, 768]               0\n",
            "     ResidualAdd-129             [-1, 197, 768]               0\n",
            "       LayerNorm-130             [-1, 197, 768]           1,536\n",
            "          Linear-131            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-132          [-1, 8, 197, 197]               0\n",
            "          Linear-133             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-134             [-1, 197, 768]               0\n",
            "         Dropout-135             [-1, 197, 768]               0\n",
            "     ResidualAdd-136             [-1, 197, 768]               0\n",
            "       LayerNorm-137             [-1, 197, 768]           1,536\n",
            "          Linear-138            [-1, 197, 3072]       2,362,368\n",
            "            GELU-139            [-1, 197, 3072]               0\n",
            "         Dropout-140            [-1, 197, 3072]               0\n",
            "          Linear-141             [-1, 197, 768]       2,360,064\n",
            "         Dropout-142             [-1, 197, 768]               0\n",
            "     ResidualAdd-143             [-1, 197, 768]               0\n",
            "       LayerNorm-144             [-1, 197, 768]           1,536\n",
            "          Linear-145            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-146          [-1, 8, 197, 197]               0\n",
            "          Linear-147             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-148             [-1, 197, 768]               0\n",
            "         Dropout-149             [-1, 197, 768]               0\n",
            "     ResidualAdd-150             [-1, 197, 768]               0\n",
            "       LayerNorm-151             [-1, 197, 768]           1,536\n",
            "          Linear-152            [-1, 197, 3072]       2,362,368\n",
            "            GELU-153            [-1, 197, 3072]               0\n",
            "         Dropout-154            [-1, 197, 3072]               0\n",
            "          Linear-155             [-1, 197, 768]       2,360,064\n",
            "         Dropout-156             [-1, 197, 768]               0\n",
            "     ResidualAdd-157             [-1, 197, 768]               0\n",
            "       LayerNorm-158             [-1, 197, 768]           1,536\n",
            "          Linear-159            [-1, 197, 2304]       1,771,776\n",
            "         Dropout-160          [-1, 8, 197, 197]               0\n",
            "          Linear-161             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-162             [-1, 197, 768]               0\n",
            "         Dropout-163             [-1, 197, 768]               0\n",
            "     ResidualAdd-164             [-1, 197, 768]               0\n",
            "       LayerNorm-165             [-1, 197, 768]           1,536\n",
            "          Linear-166            [-1, 197, 3072]       2,362,368\n",
            "            GELU-167            [-1, 197, 3072]               0\n",
            "         Dropout-168            [-1, 197, 3072]               0\n",
            "          Linear-169             [-1, 197, 768]       2,360,064\n",
            "         Dropout-170             [-1, 197, 768]               0\n",
            "     ResidualAdd-171             [-1, 197, 768]               0\n",
            "          Reduce-172                  [-1, 768]               0\n",
            "       LayerNorm-173                  [-1, 768]           1,536\n",
            "          Linear-174                 [-1, 1000]         769,000\n",
            "================================================================\n",
            "Total params: 86,415,592\n",
            "Trainable params: 86,415,592\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 364.33\n",
            "Params size (MB): 329.65\n",
            "Estimated Total Size (MB): 694.56\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(ViT().to('cuda'), (3, 224, 224), device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.10-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mMeiDzzOQO5",
        "outputId": "4fe5f25f-cdfb-448f-b5f7-fe67379a24c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: torch_xla-1.10-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6oc02fH_OS1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rco52WwS5Z9_",
        "outputId": "18da4b3d-aaaf-4c61-b543-080afa3b2990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the image\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 training and test datasets with the defined transformations\n",
        "train_dataset_full = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset_full = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Select a subset of the datasets\n",
        "train_subset = Subset(train_dataset_full, range(1000))  # First 1000 images for training\n",
        "test_subset = Subset(test_dataset_full, range(200))  # First 200 images for testing\n",
        "\n",
        "# Initialize TPU devices\n",
        "device = xm.xla_device()\n",
        "\n",
        "# Create data loaders for the subsets\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# Now you can use these loaders in your training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Your training code here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################## new method, object word\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes=10, emb_size=768, depth=12, **kwargs):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
        "        self.patch_embedding = PatchEmbedding(**kwargs)\n",
        "        self.transformer_encoder = TransformerEncoder(depth=depth, emb_size=emb_size, **kwargs)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patch_weights = calculate_patch_weights(x)\n",
        "        x = self.patch_embedding(x, patch_weights)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, 0]  # Assuming the first token is the class token\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "# Instantiate the model\n",
        "model = VisionTransformer()\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02fGSKnAi6E6",
        "outputId": "ad5d36b4-6c65-4beb-cf26-fd39ff110277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embedding): PatchEmbedding(\n",
              "    (projection): Sequential(\n",
              "      (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
              "    )\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (0): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerEncoderBlock(\n",
              "      (0): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): MultiHeadAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (att_drop): Dropout(p=0, inplace=False)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): ResidualAdd(\n",
              "        (fn): Sequential(\n",
              "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): FeedForwardBlock(\n",
              "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXnzL-vM8yDt",
        "outputId": "bfe9dd14-47b1-478b-d96f-03c83ae2e559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.11-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client\n",
        "!pip install torch-xla\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "erSew3JPLkV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes=10, emb_size=768, depth=12, **kwargs):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ViT-B/32 model\n",
        "        self.vit = timm.create_model('vit_base_patch32_224', pretrained=True)\n",
        "\n",
        "        # Replace the classifier head of the ViT model for CIFAR-10\n",
        "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
        "\n",
        "        # If using custom PatchEmbedding and TransformerEncoder, initialize them\n",
        "        self.patch_embedding = PatchEmbedding(**kwargs)\n",
        "        #self.transformer_encoder = TransformerEncoderBlock(depth=depth, emb_size=emb_size, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Shape of input to forward: \", x.shape)\n",
        "        # Forward through the pre-trained ViT model\n",
        "        x = self.vit(x)\n",
        "\n",
        "        # Uncomment and modify below lines if you're using custom patch embedding and encoder\n",
        "        print(\"Shape after ViT: \", x.shape)\n",
        "        patch_weights = calculate_patch_weights(x)\n",
        "        print(\"Shape before patch embedding: \", x.shape)\n",
        "        x = self.patch_embedding(x, patch_weights)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, 0]  # Select the class token\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "def sobel_filter(image_tensor):\n",
        "    # Ensure the input tensor is 2D: [height, width]\n",
        "    if image_tensor.dim() != 2:\n",
        "        raise ValueError(f\"Expected input tensor to be 2D, but got {image_tensor.dim()}D\")\n",
        "\n",
        "    # Sobel filter weights\n",
        "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=image_tensor.device)\n",
        "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=image_tensor.device)\n",
        "\n",
        "    # Apply Sobel filter\n",
        "    gradient_x = F.conv2d(image_tensor.unsqueeze(0).unsqueeze(0), sobel_x.unsqueeze(0).unsqueeze(0), padding=1)\n",
        "    gradient_y = F.conv2d(image_tensor.unsqueeze(0).unsqueeze(0), sobel_y.unsqueeze(0).unsqueeze(0), padding=1)\n",
        "\n",
        "    # Compute the gradient magnitude\n",
        "    gradient_magnitude = torch.sqrt(gradient_x**2 + gradient_y**2)\n",
        "\n",
        "    return gradient_magnitude[0, 0, :, :]\n",
        "\n",
        "\n",
        "# Function to calculate patch weights (placeholder)\n",
        "def calculate_patch_weights(image_tensor, patch_size=16):\n",
        "    # Calculate edges using the Sobel filter\n",
        "    edges = sobel_filter(image_tensor)\n",
        "\n",
        "    b, c, h, w = image_tensor.shape\n",
        "    edge_patches = edges.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    edge_patches = edge_patches.contiguous().view(b, 1, -1, patch_size, patch_size)  # b, 1, num_patches, patch_size, patch_size\n",
        "\n",
        "    # Compute weights for each patch based on edge intensity\n",
        "    patch_weights = edge_patches.mean(dim=[3, 4])  # Average intensity of edges in each patch\n",
        "\n",
        "    return patch_weights\n",
        "#     # Implement your method here\n",
        "#     return torch.ones((x.size(0), x.size(2) // 16, x.size(3) // 16), device=x.device)\n",
        "\n",
        "# Placeholder for PatchEmbedding and TransformerEncoder (implement these)\n",
        "# class PatchEmbedding(nn.Module):\n",
        "#modified patch embedding for new method\n",
        "\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
        "\n",
        "    def forward(self, x: Tensor, patch_weights: Tensor = None) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply patch weights if provided\n",
        "        if patch_weights is not None:\n",
        "            # Ensure patch weights are properly shaped and broadcastable\n",
        "            patch_weights = patch_weights.view(b, -1, 1)\n",
        "            x *= patch_weights  # Element-wise multiplication of weights\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.positions\n",
        "        return x\n",
        "\n",
        "\n",
        "#     ...\n",
        "\n",
        "# class TransformerEncoder(nn.Module):\n",
        "#     ...\n",
        "#Transformer Encoder\n",
        "#class TransformerEncoderBlock(nn.Sequential):\n",
        " #   def __init__(self,\n",
        "  #               emb_size: int = 768,\n",
        "   #              drop_p: float = 0.,\n",
        "    #             forward_expansion: int = 4,\n",
        "     #            forward_drop_p: float = 0.,\n",
        "      #           ** kwargs):\n",
        "       # super().__init__(\n",
        "        #    ResidualAdd(nn.Sequential(\n",
        "         #       nn.LayerNorm(emb_size),\n",
        "          #      MultiHeadAttention(emb_size, **kwargs),\n",
        "           #     nn.Dropout(drop_p)\n",
        "            #)),\n",
        "            #ResidualAdd(nn.Sequential(\n",
        "             #   nn.LayerNorm(emb_size),\n",
        "              #  FeedForwardBlock(\n",
        "               #     emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "               # nn.Dropout(drop_p)\n",
        "            #)\n",
        "            #))\n",
        "\n",
        "\n",
        "# Instantiate model and move it to TPU\n",
        "device = xm.xla_device()\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "# Assuming the criterion and optimizer are already defined\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        xm.mark_step()  # Mark the step for TPU execution\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "JvpEhLy78thA",
        "outputId": "c39d21d6-6802-4a01-84c5-48ddb986c7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input to forward:  torch.Size([64, 3, 224, 224])\n",
            "Shape after ViT:  torch.Size([64, 10])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6ccabfeea221>\u001b[0m in \u001b[0;36m<cell line: 147>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6ccabfeea221>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Uncomment and modify below lines if you're using custom patch embedding and encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape after ViT: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpatch_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_patch_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape before patch embedding: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6ccabfeea221>\u001b[0m in \u001b[0;36mcalculate_patch_weights\u001b[0;34m(image_tensor, patch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msobel_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0medge_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0medge_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_patches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b, 1, num_patches, patch_size, patch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60E9WqwWndBQ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the criterion and optimizer are already defined\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5  # Define the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    # Optional: Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(test_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pWNn5-1O9QoY",
        "outputId": "fee80841-8661-496d-dc38-51cba28e60fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-60f2f4a212d4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-8c02d98f8dd9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Forward through the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50176x224 and 768x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo5CrV9AnftY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6117b7be-fbdf-4ab8-de2e-05627be22f89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b07e0665603f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-5439f725d01e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Use ViT's classifier head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# If using a custom classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14336x224 and 768x10)"
          ]
        }
      ],
      "source": [
        "num_epochs = 5  # or any other value\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"/content/drive/My Drive/vit_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "PnBS_lKZLnV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ydq3YgAml0lU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508e16e2-8f63-4196-ce0e-16765092b0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on test images: 23.17%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to('cuda')  # Move images to GPU\n",
        "        labels = labels.to('cuda')  # Move labels to GPU\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on test images: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0wgz2hw4me_F"
      },
      "outputs": [],
      "source": [
        "#model_save_path = \"/content/drive/My Drive/vit_model.pth\"\n",
        "#torch.save(model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Select whether you would like to store data in your personal drive.\n",
        "#@markdown\n",
        "#@markdown If you select **yes**, you will need to authorize Colab to access\n",
        "#@markdown your personal drive\n",
        "#@markdown\n",
        "#@markdown If you select **no**, then any changes you make will diappear when\n",
        "#@markdown this Colab's VM restarts after some time of inactivity...\n",
        "use_gdrive = 'no'  #@param [\"yes\", \"no\"]\n",
        "\n",
        "if use_gdrive == 'yes':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  root = '/gdrive/My Drive/vision_transformer_colab'\n",
        "  import os\n",
        "  if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "  os.chdir(root)\n",
        "  print(f'\\nChanged CWD to \"{root}\"')\n",
        "else:\n",
        "  from IPython import display\n",
        "  display.display(display.HTML(\n",
        "      '<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "E9nnBI4ldUJ9",
        "outputId": "5602a467-228e-4a74-b76f-366b927e0e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repository and pull latest changes.\n",
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n",
        "!cd vision_transformer && git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DmKWXw6eXFs",
        "outputId": "57d2e5fe-590f-49b1-ee4b-9e1844a97a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qr vision_transformer/vit_jax/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGwrPvameave",
        "outputId": "3a2156b4-bc70-4c8c-fc67-79b497078ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.2/801.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jax 0.4.20 does not provide the extra 'cuda11_cudnn82'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.2/198.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.2/198.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.6/186.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.4/69.4 kB\u001b[0m \u001b[31m401.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.2/135.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.6/205.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.9/204.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 409, in resolve\n",
            "    unsatisfied_names = [\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 412, in <listcomp>\n",
            "    if not self._is_current_pin_satisfying(key, criterion)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 222, in _is_current_pin_satisfying\n",
            "    return all(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 223, in <genexpr>\n",
            "    self._p.is_satisfied_by(requirement=r, candidate=current_pin)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 240, in is_satisfied_by\n",
            "    return requirement.is_satisfied_by(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/requirements.py\", line 132, in is_satisfied_by\n",
            "    return self.specifier.contains(candidate.version, prereleases=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 749, in contains\n",
            "    return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 749, in <genexpr>\n",
            "    return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 189, in contains\n",
            "    return operator_callable(normalized_item, self.version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 294, in wrapped\n",
            "    return fn(self, prospective, spec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 489, in _compare_greater_than_equal\n",
            "    return Version(prospective.public) >= Version(spec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/version.py\", line 269, in __init__\n",
            "    self._version = _Version(\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 283, in __init__\n",
            "    def __init__(self, name, level, pathname, lineno,\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a pre-trained model.\n",
        "\n",
        "# Note: you can really choose any of the above, but this Colab has been tested\n",
        "# with the models of below selection...\n",
        "model_name = 'ViT-B_32'  #@param [\"ViT-B_32\", \"Mixer-B_16\"]\n",
        "\n",
        "if model_name.startswith('ViT'):\n",
        "  ![ -e \"$model_name\".npz ] || gsutil cp gs://vit_models/imagenet21k/\"$model_name\".npz .\n",
        "if model_name.startswith('Mixer'):\n",
        "  ![ -e \"$model_name\".npz ] || gsutil cp gs://mixer_models/imagenet21k/\"$model_name\".npz .\n",
        "\n",
        "import os\n",
        "assert os.path.exists(f'{model_name}.npz')"
      ],
      "metadata": {
        "id": "CRxsTXPtedLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab \"TPU\" runtimes are configured in \"2VM mode\", meaning that JAX\n",
        "# cannot see the TPUs because they're not directly attached. Instead we need to\n",
        "# setup JAX to communicate with a second machine that has the TPUs attached.\n",
        "import os\n",
        "if 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n",
        "  import jax\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "  print('Connected to TPU.')\n",
        "else:\n",
        "  print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C70N8rDehkR",
        "outputId": "4fc9ce29-7b70-4aee-ccf2-34e37861dbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No TPU detected. Can be changed under \"Runtime/Change runtime type\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from absl import logging\n",
        "import flax\n",
        "import jax\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "# Shows the number of available devices.\n",
        "# In a CPU/GPU runtime this will be a single device.\n",
        "# In a TPU runtime this will be 8 cores.\n",
        "jax.local_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMmElzqZejsQ",
        "outputId": "5365c793-c9be-4167-b62d-cc7ecd869b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[cuda(id=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open some code files in a split editor on the right.\n",
        "# You can open more files in the file tab on the left.\n",
        "from google.colab import files\n",
        "files.view('vision_transformer/vit_jax/configs/common.py')\n",
        "files.view('vision_transformer/vit_jax/configs/models.py')\n",
        "files.view('vision_transformer/vit_jax/checkpoint.py')\n",
        "files.view('vision_transformer/vit_jax/input_pipeline.py')\n",
        "files.view('vision_transformer/vit_jax/models.py')\n",
        "files.view('vision_transformer/vit_jax/train.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rTUXiEPvellf",
        "outputId": "807ee1ed-1ab8-436a-d9f7-67fd0bea8f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/configs/common.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/configs/models.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/checkpoint.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/input_pipeline.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/models.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/train.py\")"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import files from repository.\n",
        "# Updating the files in the editor on the right will immediately update the\n",
        "# modules by re-importing them.\n",
        "\n",
        "import sys\n",
        "if './vision_transformer' not in sys.path:\n",
        "  sys.path.append('./vision_transformer')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from vit_jax import checkpoint\n",
        "from vit_jax import input_pipeline\n",
        "from vit_jax import utils\n",
        "from vit_jax import models\n",
        "from vit_jax import train\n",
        "from vit_jax.configs import common as common_config\n",
        "from vit_jax.configs import models as models_config"
      ],
      "metadata": {
        "id": "LtvGb-TAf7yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions for images.\n",
        "\n",
        "labelnames = dict(\n",
        "  # https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "  cifar10=('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n",
        "  # https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "  cifar100=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
        ")\n",
        "def make_label_getter(dataset):\n",
        "  \"\"\"Returns a function converting label indices to names.\"\"\"\n",
        "  def getter(label):\n",
        "    if dataset in labelnames:\n",
        "      return labelnames[dataset][label]\n",
        "    return f'label={label}'\n",
        "  return getter\n",
        "\n",
        "def show_img(img, ax=None, title=None):\n",
        "  \"\"\"Shows a single image.\"\"\"\n",
        "  if ax is None:\n",
        "    ax = plt.gca()\n",
        "  ax.imshow(img[...])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  if title:\n",
        "    ax.set_title(title)\n",
        "\n",
        "def show_img_grid(imgs, titles):\n",
        "  \"\"\"Shows a grid of images.\"\"\"\n",
        "  n = int(np.ceil(len(imgs)**.5))\n",
        "  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n",
        "  for i, (img, title) in enumerate(zip(imgs, titles)):\n",
        "    img = (img + 1) / 2  # Denormalize\n"
      ],
      "metadata": {
        "id": "36Yerq_1gD6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'cifar10'\n",
        "batch_size = 512\n",
        "config = common_config.with_dataset(common_config.get_config(), dataset)\n",
        "config.batch = batch_size\n",
        "config.pp.crop = 224"
      ],
      "metadata": {
        "id": "7mYki1-ZgF9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For details about setting up datasets, see input_pipeline.py on the right.\n",
        "ds_train = input_pipeline.get_data_from_tfds(config=config, mode='train')\n",
        "ds_test = input_pipeline.get_data_from_tfds(config=config, mode='test')\n",
        "num_classes = input_pipeline.get_dataset_info(dataset, 'train')['num_classes']\n",
        "del config  # Only needed to instantiate datasets."
      ],
      "metadata": {
        "id": "M16f4io9gIRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch a batch of test images for illustration purposes.\n",
        "batch = next(iter(ds_test.as_numpy_iterator()))\n",
        "# Note the shape : [num_local_devices, local_batch_size, h, w, c]\n",
        "batch['image'].shape"
      ],
      "metadata": {
        "id": "8sMN0lD4gKM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = models_config.MODEL_CONFIGS[model_name]\n",
        "model_config"
      ],
      "metadata": {
        "id": "3dZNPwnbgMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model definition & initialize random parameters.\n",
        "# This also compiles the model to XLA (takes some minutes the first time).\n",
        "if model_name.startswith('Mixer'):\n",
        "  model = models.MlpMixer(num_classes=num_classes, **model_config)\n",
        "else:\n",
        "  model = models.VisionTransformer(num_classes=num_classes, **model_config)\n",
        "variables = jax.jit(lambda: model.init(\n",
        "    jax.random.PRNGKey(0),\n",
        "    # Discard the \"num_local_devices\" dimension of the batch for initialization.\n",
        "    batch['image'][0, :1],\n",
        "    train=False,\n",
        "), backend='cpu')()"
      ],
      "metadata": {
        "id": "HorXzcFjgOiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and convert pretrained checkpoint.\n",
        "# This involves loading the actual pre-trained model results, but then also also\n",
        "# modifying the parameters a bit, e.g. changing the final layers, and resizing\n",
        "# the positional embeddings.\n",
        "# For details, refer to the code and to the methods of the paper.\n",
        "params = checkpoint.load_pretrained(\n",
        "    pretrained_path=f'{model_name}.npz',\n",
        "    init_params=variables['params'],\n",
        "    model_config=model_config,\n",
        ")"
      ],
      "metadata": {
        "id": "cGY4y2L3gQw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate\n",
        "# So far, all our data is in the host memory. Let's now replicate the arrays\n",
        "# into the devices.\n",
        "# This will make every array in the pytree params become a ShardedDeviceArray\n",
        "# that has the same data replicated across all local devices.\n",
        "# For TPU it replicates the params in every core.\n",
        "# For a single GPU this simply moves the data onto the device.\n",
        "# For CPU it simply creates a copy.\n",
        "params_repl = flax.jax_utils.replicate(params)\n",
        "print('params.cls:', type(params['head']['bias']).__name__,\n",
        "      params['head']['bias'].shape)\n",
        "print('params_repl.cls:', type(params_repl['head']['bias']).__name__,\n",
        "      params_repl['head']['bias'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3DSw4hxfDM7",
        "outputId": "f547566c-82ce-4fe9-f75f-a6bc6031b6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then map the call to our model's forward pass onto all available devices.\n",
        "vit_apply_repl = jax.pmap(lambda params, inputs: model.apply(\n",
        "    dict(params=params), inputs, train=False))"
      ],
      "metadata": {
        "id": "gI88LrsfhWwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(params_repl):\n",
        "  \"\"\"Returns accuracy evaluated on the test set.\"\"\"\n",
        "  good = total = 0\n",
        "  steps = input_pipeline.get_dataset_info(dataset, 'test')['num_examples'] // batch_size\n",
        "  for _, batch in zip(tqdm.trange(steps), ds_test.as_numpy_iterator()):\n",
        "    predicted = vit_apply_repl(params_repl, batch['image'])\n",
        "    is_same = predicted.argmax(axis=-1) == batch['label'].argmax(axis=-1)\n",
        "    good += is_same.sum()\n",
        "    total += len(is_same.flatten())\n",
        "  return good / total"
      ],
      "metadata": {
        "id": "MvG-wVAUhYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine Tune\n",
        "# 100 Steps take approximately 15 minutes in the TPU runtime.\n",
        "total_steps = 100\n",
        "warmup_steps = 5\n",
        "decay_type = 'cosine'\n",
        "grad_norm_clip = 1\n",
        "# This controls in how many forward passes the batch is split. 8 works well with\n",
        "# a TPU runtime that has 8 devices. 64 should work on a GPU. You can of course\n",
        "# also adjust the batch_size above, but that would require you to adjust the\n",
        "# learning rate accordingly.\n",
        "accum_steps = 8\n",
        "base_lr = 0.03"
      ],
      "metadata": {
        "id": "qRJ2c0Kmhaif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out train.make_update_fn in the editor on the right side for details.\n",
        "lr_fn = utils.create_learning_rate_schedule(total_steps, base_lr, decay_type, warmup_steps)\n",
        "# We use a momentum optimizer that uses half precision for state to save\n",
        "# memory. It als implements the gradient clipping.\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(grad_norm_clip),\n",
        "    optax.sgd(\n",
        "        learning_rate=lr_fn,\n",
        "        momentum=0.9,\n",
        "        accumulator_dtype='bfloat16',\n",
        "    ),\n",
        ")\n",
        "update_fn_repl = train.make_update_fn(\n",
        "    apply_fn=model.apply, accum_steps=accum_steps, tx=tx)\n",
        "opt_state = tx.init(params)\n",
        "opt_state_repl = flax.jax_utils.replicate(opt_state)"
      ],
      "metadata": {
        "id": "NSEilBvzhcoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PRNGs for dropout.\n",
        "update_rng_repl = flax.jax_utils.replicate(jax.random.PRNGKey(0))"
      ],
      "metadata": {
        "id": "ZK4MaaRPhed4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "lrs = []\n",
        "# Completes in ~20 min on the TPU runtime.\n",
        "for step, batch in zip(\n",
        "    tqdm.trange(1, total_steps + 1),\n",
        "    ds_train.as_numpy_iterator(),\n",
        "):\n",
        "\n",
        "  params_repl, opt_state_repl, loss_repl, update_rng_repl = update_fn_repl(\n",
        "      params_repl, opt_state_repl, batch, update_rng_repl)\n",
        "  losses.append(loss_repl[0])\n",
        "  lrs.append(lr_fn(step))\n",
        "\n"
      ],
      "metadata": {
        "id": "SKr6XX5mhhBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should be ~96.7% for Mixer-B/16 or 97.7% for ViT-B/32 on CIFAR10 (both @224)\n",
        "get_accuracy(params_repl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DKmt21tfX1R",
        "outputId": "28c819c3-63af-4cb6-d4f1-d73b7b30759c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Add new layers for CIFAR-10 classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n2gRNfffc8D",
        "outputId": "72de2753-fae3-4145-fdf6-1f8be787542b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 11s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 33s 14ms/step - loss: 1.1848 - accuracy: 0.6081 - val_loss: 1.0213 - val_accuracy: 0.6469\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.9063 - accuracy: 0.6826 - val_loss: 1.0200 - val_accuracy: 0.6506\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.7972 - accuracy: 0.7188 - val_loss: 1.0679 - val_accuracy: 0.6485\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.6739 - accuracy: 0.7616 - val_loss: 1.0912 - val_accuracy: 0.6586\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.5617 - accuracy: 0.8015 - val_loss: 1.2318 - val_accuracy: 0.6483\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.2318 - accuracy: 0.6483\n",
            "Test Accuracy: 64.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-92o6brlNYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "173u7mGvtPHNAQZYaVvSV2IG4jeAUa0Ln",
      "authorship_tag": "ABX9TyM3uLtBVmYBMyHNnfbHWFAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}